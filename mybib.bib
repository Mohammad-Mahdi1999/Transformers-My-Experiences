@article{Multimodal_survey,
	author    = {Peng Xu},
	title     = {Multimodal Learning With Transformers: A Survey},
	journal   = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
	year      = {2023},
	volume    = {45},
	number    = {10},
	publisher = {IEEE},
}

@article{bert,
	author    = {J. Devlin et al.},
	title     = {BERT: Pretraining of deep bidirectional transformers for language understanding},
	year      = {2018},
	publisher = {arXiv},
}
@article{BART,
	author    = {M. Lewis et al.},
	title     = {BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	year      = {2019},
	publisher = {arXiv},
}
@article{gpt,
	author    = {A. Radford et al.},
	title     = {Improving
	language understanding by generative pre-training},
	link = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf},
}
@article{Longformer,
	author    = {I. Beltagy},
	title     = {Longformer: The long-document
	transformer},
	year      = {2020},
	publisher = {arXiv},
}
@article{Transformer-XL,
	author    = {Z. Dai},
	title     = {Transformer-XL: Attentive language models beyond a fixed-length context},
	year      = {2019},
	publisher = {arXiv},
}
@article{XLNet,
author    = {Z. Yang},
title     = {XLNet: Generalized autoregressive pretraining for language
understanding},
year      = {2019},
publisher = {Proc. Int. Conf. Neural Inf. Process. Syst.},
}